The CRAN package doRedis is a foreach plugin that uses the lightweight
redis key-value database to coordinate the parallel evaluation of
tasks.  The foreach function operates in two models, %do% and
%dopar%. The %do% mode executes tasks serially while %dopar% executes
in parallel. The %do% mode is easier to debug. Once the R code works
with serial execution then it can be converted to parallel mode with
the %dopar% operator.

Parallel mode requires the redis database engine. Before attempting to
execute tasks in the cloud, there is an opportunity for further
debugging on the local system. On Debian, the redis database is easy
to install and there is no additional configuration necessary beyond
'apt install redis'. By default, redis creates some empty databases
and allows connections from localhost.

For parallel model on the local host, we call
doRedis::startLocalWorkers(n=2). This function forks to presents us
with exactly the same execution environment that we would face in the
cloud.  Debugging can be tricky because doRedis is not thoroughly
meticulous about reporting error conditions. If data can be fit into
RAM then it can be efficient to load it once at the beginning of
analysis. We need to use global variables because foreach otherwise
provides a fresh environment. This can be accomplish by code similar
to,

if (!("dt_l" %in% ls(envir=globalenv()))) {
  print("loading data")
  load("/opt/DynamicHeritability2.RData"), envir=globalenv())
  print("got data")

  rm(dt_r, envir=globalenv()) # REMOVES CONTRALATERAL VERTEX-LEVEL DATA FROM WORKSPACE TO FREE MEMORY
}
 lapply(c('dt_l','demo'), function(n)
      assign(n, get(n, envir=globalenv())))

Kubernetes is an open-source cloud service abstraction layer that
supervises provisioning and execution of cloud services regardless of
the cloud provider. Kubernetes can talk to a variety of cloud
providers including Google, Amazon, and many others. By using
Kubernetes, we avoid getting lock into a proprietary cloud services
API of a particular vendor.

Kubernetes involves a variety of specialized terminology. Containers
are the finest indivisible unit. Containers act like virtual machines
and run Docker images.  Docker does lightweight virtualization. Docker
images consist of a stack of tar files that describe the content of
the file systems along with some meta information. Kubernetes
organizes a set of containers into a pod. Pods are assigned to
nodes. Nodes represent physical hardware, but are actually virtualized
hardware. A cluster consists of a set of nodes. Clusters are created
using a vendor specific API and Kubernetes supervises what goes on
within a cluster.

There are already ready-made docker images for the redis database. To
launch redis with Kubernetes is a simple affair. However, we also need
a docker image to act as a worker in our parallel computation. For
this, we need to put together a custom docker image. The image needs
to start non-interactively and immediate offer its services. This is
accomplished by placing code in /root/.Rprofile and setting the image
to run 'R --no-save' on start up. Whatever CRAN packages are required
should also be pre-installed to minimize spin-up time.

Now we are ready. Create a cluster using your vendor's API. In Google
cloud services (GCS), there is a web interface. GCS has separate
quotas for number of CPUs and number of unique IPs. Therefore, to
reach full utilization, you'll need to request the appropriate number
of CPUs per virtual machine. There is no need to create more than 2
virtual machines to start because the cluster can be dynamically
resized. GCS can be told to start Kubernetes automaticaly. We start
our services using kubectl,

kubectl create -f redis.yaml
kubectl create -f redis_service.yaml
kubectl create -f rworker.yaml

This results in the following services running,

kubectl get all

NAME                  READY     STATUS    RESTARTS   AGE
po/redis-svc-ggcnw    1/1       Running   0          5m
po/rworker-rc-g196d   1/1       Running   0          1m

NAME            DESIRED   CURRENT   READY     AGE
rc/redis-svc    1         1         1         5m
rc/rworker-rc   1         1         1         1m

We have a worker running, but we still need to kick off the
computation. This is accomplished with a few manual steps. Set a shell
variable to the name of the first worker,

wk=rworker-rc-3fxj6

In addition to working, this node will also distribute work and collect
results. We copy our script over and open an ssh session to the node,

kubectl cp template.R $wk:/root/
kubectl exec $wk -it bash

Then we run our main script,

nohup R --no-init-file --no-save -f template.R &

The '--no-init-file' option is necessary to avoid /root/.Rprofile. The
.Rprofile will set up R to be a worker non-interactively so we need to
avoid that. At this point, ensure that the computation is proceeding
correctly. Console output can be monitored with,

kubectl logs -f rworker-rc-xxxxx

Once it is confirmed that things are proceeding smoothly and according
to plan then the parallelism can be increased dynamically. For GCS,
the command is something like,

gcloud container clusters resize cluster-1 --size=16   # for 16 nodes

Once this command completes then we tell Kubernetes to do its thing,

kubectl scale --replicas=32 rc/rworker-rc   # for 32 workers

You probably want more workers than nodes because each node offers
more than one CPU. For really big computations, you probably want to
progressively backup the results. Cloud machines are usually reliable,
but natural disasters can happen. Backups will ensure that it is
relatively painless to resume after interruption.

As soon as the computation is done, ensure that the cluster is
shutdown to stop the billing clock.
